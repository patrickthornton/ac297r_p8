{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n"
      ],
      "metadata": {
        "id": "sg_WB762DpGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIzZVtJZDiz7"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# two_view_bmi_res101_res18_letterbox_no_proj_with_buckets_meanstd.py\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from typing import List, Dict\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "from scipy.stats import ks_2samp, wasserstein_distance\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from transformers import pipeline\n",
        "\n",
        "# ----------------------------- CONFIG -------------------------------\n",
        "BASE_DIR         = \"/content/drive/MyDrive\"\n",
        "SAMPLE_JSON_PATH = os.path.join(BASE_DIR, \"sample_list.json\")\n",
        "RESNET101_PATH   = os.path.join(BASE_DIR, \"resnet101-63fe2227.pth\")\n",
        "RESNET18_PATH    = os.path.join(BASE_DIR, \"resnet18-5c106cde.pth\")\n",
        "HUMAN_NPZ_PATH   = os.path.join(BASE_DIR, \"human_mask_cache_224.npz\")\n",
        "REF_NPZ_PATH     = os.path.join(BASE_DIR, \"ref_mask_cache_224.npz\")\n",
        "REF_LABELS       = [\"a dark paper\", \"a white ball\"]\n",
        "DEVICE           = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "OUT_W, OUT_H     = 224, 224\n",
        "\n",
        "# ---------------------- LETTERBOX MASK -----------------------------\n",
        "def letterbox_mask(mask: np.ndarray, new_size=(OUT_W, OUT_H)) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Resize a binary mask to new_size with letterbox padding to preserve aspect ratio.\n",
        "    \"\"\"\n",
        "    h, w = mask.shape\n",
        "    nw, nh = new_size\n",
        "    scale = min(nw / w, nh / h)\n",
        "    rw, rh = int(w * scale), int(h * scale)\n",
        "    resized = cv2.resize(mask, (rw, rh), interpolation=cv2.INTER_NEAREST)\n",
        "    top = (nh - rh) // 2\n",
        "    left = (nw - rw) // 2\n",
        "    output = np.zeros((nh, nw), dtype=mask.dtype)\n",
        "    output[top:top+rh, left:left+rw] = resized\n",
        "    return output\n",
        "\n",
        "# ---------------------- BUILD MASK CACHES --------------------------\n",
        "def build_mask_caches(samples: List[Dict],\n",
        "                      person_pred: DefaultPredictor,\n",
        "                      ref_det,\n",
        "                      ref_labels: List[str]) -> None:\n",
        "    \"\"\"\n",
        "    Generate and cache letterboxed human and reference masks for the first two views.\n",
        "    \"\"\"\n",
        "    if os.path.exists(HUMAN_NPZ_PATH) and os.path.exists(REF_NPZ_PATH):\n",
        "        return\n",
        "    human_cache, ref_cache = {}, {}\n",
        "    for sample in tqdm(samples, desc=\"Building mask caches\"):\n",
        "        for path in sample[\"img_paths\"][:2]:\n",
        "            if path in human_cache or not os.path.exists(path):\n",
        "                continue\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "            img = ImageOps.exif_transpose(img)\n",
        "            arr = np.array(img)\n",
        "            H, W = arr.shape[:2]\n",
        "\n",
        "            # human mask via Detectron2\n",
        "            inst = person_pred(cv2.cvtColor(arr, cv2.COLOR_RGB2BGR))[\"instances\"]\n",
        "            hm = np.zeros((H, W), dtype=np.uint8)\n",
        "            if len(inst):\n",
        "                cls = inst.pred_classes.cpu().numpy()\n",
        "                ids = np.where(cls == 0)[0]\n",
        "                if len(ids):\n",
        "                    idx = ids[inst.scores.cpu().numpy()[ids].argmax()]\n",
        "                    if inst.has(\"pred_masks\"):\n",
        "                        hm = inst.pred_masks[idx].cpu().numpy().astype(np.uint8)\n",
        "                    else:\n",
        "                        y0,x0,y1,x1 = inst.pred_boxes[idx].tensor.cpu().numpy()[0]\n",
        "                        hm[int(y0):int(y1), int(x0):int(x1)] = 1\n",
        "            human_cache[path] = letterbox_mask(hm)\n",
        "\n",
        "            # reference mask via Groundingâ€‘DINO\n",
        "            dets = ref_det(img, candidate_labels=ref_labels, threshold=0.7)\n",
        "            det_map = {d[\"label\"].lower(): d for d in dets}\n",
        "            for lbl in ref_labels:\n",
        "                key = f\"{path}|{lbl}\"\n",
        "                rm = np.zeros((H, W), dtype=np.uint8)\n",
        "                d  = det_map.get(lbl)\n",
        "                if d:\n",
        "                    x0,y0 = int(d[\"box\"][\"xmin\"]), int(d[\"box\"][\"ymin\"])\n",
        "                    x1,y1 = int(d[\"box\"][\"xmax\"]), int(d[\"box\"][\"ymax\"])\n",
        "                    rm[y0:y1, x0:x1] = 1\n",
        "                ref_cache[key] = letterbox_mask(rm)\n",
        "\n",
        "    np.savez_compressed(HUMAN_NPZ_PATH, **human_cache)\n",
        "    np.savez_compressed(REF_NPZ_PATH,   **ref_cache)\n",
        "\n",
        "# -------------------------- DATASET -------------------------------\n",
        "class TwoViewMaskDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset that returns two RGB images plus their human/ref masks and BMI.\n",
        "    \"\"\"\n",
        "    def __init__(self, samples, resize, tf, ref_labels):\n",
        "        self.samples    = samples\n",
        "        self.resize     = resize\n",
        "        self.tf         = tf\n",
        "        self.ref_labels = ref_labels\n",
        "        self.h_masks    = dict(np.load(HUMAN_NPZ_PATH, allow_pickle=True))\n",
        "        self.r_masks    = dict(np.load(REF_NPZ_PATH,   allow_pickle=True))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = self.samples[idx]\n",
        "        p1, p2 = s[\"img_paths\"][:2]\n",
        "        img1 = Image.open(p1).convert(\"RGB\")\n",
        "        img2 = Image.open(p2).convert(\"RGB\")\n",
        "        rgb1 = self.tf(self.resize(img1))\n",
        "        rgb2 = self.tf(self.resize(img2))\n",
        "        m1 = torch.from_numpy(self.h_masks[p1]).unsqueeze(0).float()\n",
        "        m2 = torch.from_numpy(self.h_masks[p2]).unsqueeze(0).float()\n",
        "        rm1 = np.stack([self.r_masks[f\"{p1}|{lbl}\"] for lbl in self.ref_labels], 0)\n",
        "        rm2 = np.stack([self.r_masks[f\"{p2}|{lbl}\"] for lbl in self.ref_labels], 0)\n",
        "        rm1 = torch.from_numpy(rm1).float()\n",
        "        rm2 = torch.from_numpy(rm2).float()\n",
        "        bmi = torch.tensor(float(s[\"bmi\"]), dtype=torch.float32).unsqueeze(0)\n",
        "        return rgb1, rgb2, m1, m2, rm1, rm2, bmi\n",
        "\n",
        "# --------------------------- MODEL -------------------------------\n",
        "class MultiBranchNoProj(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-branch model:\n",
        "      - RGB branch: ResNet-101\n",
        "      - Human-mask branch: ResNet-18\n",
        "      - Ref-mask branch: ResNet-18\n",
        "      Outputs regression and classification heads.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_ref=2):\n",
        "        super().__init__()\n",
        "        # RGB backbone\n",
        "        r101 = models.resnet101(weights=None)\n",
        "        sd = torch.load(RESNET101_PATH, map_location=\"cpu\", weights_only=False)\n",
        "        r101.load_state_dict(sd, strict=False)\n",
        "        self.rgb_backbone = nn.Sequential(*list(r101.children())[:-1])\n",
        "        # Human-mask backbone\n",
        "        h18 = models.resnet18(weights=None)\n",
        "        sd = torch.load(RESNET18_PATH, map_location=\"cpu\", weights_only=False)\n",
        "        h18.load_state_dict(sd, strict=False)\n",
        "        w0 = h18.conv1.weight.data\n",
        "        h18.conv1 = nn.Conv2d(1,64,7,2,3,bias=False)\n",
        "        h18.conv1.weight.data = w0.mean(dim=1,keepdim=True)\n",
        "        self.h_backbone = nn.Sequential(*list(h18.children())[:-1])\n",
        "        # Ref-mask backbone\n",
        "        r18 = models.resnet18(weights=None)\n",
        "        sd = torch.load(RESNET18_PATH, map_location=\"cpu\", weights_only=False)\n",
        "        r18.load_state_dict(sd, strict=False)\n",
        "        w1 = r18.conv1.weight.data.mean(dim=1,keepdim=True)\n",
        "        r18.conv1 = nn.Conv2d(num_ref,64,7,2,3,bias=False)\n",
        "        r18.conv1.weight.data = w1.repeat(1,num_ref,1,1)\n",
        "        self.r_backbone = nn.Sequential(*list(r18.children())[:-1])\n",
        "        # Heads\n",
        "        in_dim = (2048 + 512 + 512) * 2\n",
        "        self.fc_reg = nn.Linear(in_dim, 1)\n",
        "        self.fc_cls = nn.Linear(in_dim, 3)\n",
        "\n",
        "    def forward(self, rgb1, rgb2, hm1, hm2, rm1, rm2):\n",
        "        def feat(backbone, x): return backbone(x).flatten(1)\n",
        "        r1 = feat(self.rgb_backbone, rgb1)\n",
        "        r2 = feat(self.rgb_backbone, rgb2)\n",
        "        h1 = feat(self.h_backbone,   hm1)\n",
        "        h2 = feat(self.h_backbone,   hm2)\n",
        "        f1 = feat(self.r_backbone,    rm1)\n",
        "        f2 = feat(self.r_backbone,    rm2)\n",
        "        v1 = torch.cat([r1,h1,f1], dim=1)\n",
        "        v2 = torch.cat([r2,h2,f2], dim=1)\n",
        "        fused = torch.cat([v1,v2], dim=1)\n",
        "        return self.fc_reg(fused), self.fc_cls(fused)\n",
        "\n",
        "# ---------------------- UTILITIES -------------------------------\n",
        "def get_bmi_class_new(bmi: float) -> int:\n",
        "    if bmi < 17.28: return 0\n",
        "    if bmi < 25.57: return 1\n",
        "    return 2\n",
        "\n",
        "def compute_metrics(gt, pred):\n",
        "    gt, pred = np.asarray(gt), np.asarray(pred)\n",
        "    err = pred - gt\n",
        "    ks, _ = ks_2samp(gt, pred)\n",
        "    return {\n",
        "        \"r2\":          r2_score(gt, pred),\n",
        "        \"mae\":         mean_absolute_error(gt, pred),\n",
        "        \"tol_rate\":    np.mean(np.abs(err) <= 1.0),\n",
        "        \"mean_bias\":   err.mean(),\n",
        "        \"error_std\":   err.std(),\n",
        "        \"ks\":          ks,\n",
        "        \"wasserstein\": wasserstein_distance(gt, pred),\n",
        "    }\n",
        "\n",
        "# -------------------------- MAIN -------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    with open(SAMPLE_JSON_PATH) as f:\n",
        "        samples = json.load(f)\n",
        "\n",
        "    # build caches on first run\n",
        "    BUILD_CACHE = False\n",
        "    if BUILD_CACHE:\n",
        "        cfg = get_cfg()\n",
        "        cfg.merge_from_file(model_zoo.get_config_file(\n",
        "            \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "            \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "        human_pred = DefaultPredictor(cfg)\n",
        "        ref_det    = pipeline(\"zero-shot-object-detection\",\n",
        "                              model=\"IDEA-Research/grounding-dino-tiny\",\n",
        "                              device=0 if torch.cuda.is_available() else -1)\n",
        "        build_mask_caches(samples, human_pred, ref_det, REF_LABELS)\n",
        "        exit(0)\n",
        "\n",
        "    # split data\n",
        "    train = [s for s in samples if s[\"split\"] == \"Training\"]\n",
        "    val   = [s for s in samples if s[\"split\"] == \"Validation\"]\n",
        "\n",
        "    # data loaders\n",
        "    resize = transforms.Resize((OUT_H, OUT_W))\n",
        "    tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "    ])\n",
        "    train_loader = DataLoader(\n",
        "        TwoViewMaskDataset(train, resize, tf, REF_LABELS),\n",
        "        batch_size=8, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(\n",
        "        TwoViewMaskDataset(val,   resize, tf, REF_LABELS),\n",
        "        batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # training settings\n",
        "    CHECKPOINTS = [20, 25, 30, 35]\n",
        "    NUM_RUNS    = 3\n",
        "    EPOCHS      = 35\n",
        "\n",
        "    # prepare accumulators for metrics and bucket MAE\n",
        "    overall_results = {ep: {k: [] for k in compute_metrics([0],[0])} for ep in CHECKPOINTS}\n",
        "    bucket_results  = {ep: {0:[], 1:[], 2:[]} for ep in CHECKPOINTS}\n",
        "\n",
        "    for run in range(1, NUM_RUNS+1):\n",
        "        print(f\"\\n===== RUN {run} =====\")\n",
        "        model     = MultiBranchNoProj(len(REF_LABELS)).to(DEVICE)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "        mse_loss  = nn.MSELoss()\n",
        "\n",
        "        # train for EPOCHS\n",
        "        for epoch in range(1, EPOCHS+1):\n",
        "            model.train()\n",
        "            for rgb1,rgb2,hm1,hm2,rm1,rm2,bmi in train_loader:\n",
        "                rgb1,rgb2,hm1,hm2,rm1,rm2,bmi = [\n",
        "                    x.to(DEVICE) for x in (rgb1,rgb2,hm1,hm2,rm1,rm2,bmi)\n",
        "                ]\n",
        "                labels = torch.tensor(\n",
        "                    [get_bmi_class_new(b.item()) for b in bmi.flatten()],\n",
        "                    dtype=torch.long, device=DEVICE\n",
        "                )\n",
        "                optimizer.zero_grad()\n",
        "                out_reg, out_cls = model(rgb1,rgb2,hm1,hm2,rm1,rm2)\n",
        "                loss = mse_loss(out_reg, bmi)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # evaluate at checkpoints\n",
        "            if epoch in CHECKPOINTS:\n",
        "                model.eval()\n",
        "                all_preds, all_gts, all_buckets = [], [], []\n",
        "                with torch.no_grad():\n",
        "                    for rgb1,rgb2,hm1,hm2,rm1,rm2,bmi in val_loader:\n",
        "                        rgb1,rgb2,hm1,hm2,rm1,rm2 = [\n",
        "                            x.to(DEVICE) for x in (rgb1,rgb2,hm1,hm2,rm1,rm2)\n",
        "                        ]\n",
        "                        out_reg, _ = model(rgb1,rgb2,hm1,hm2,rm1,rm2)\n",
        "                        preds = out_reg.cpu().flatten().tolist()\n",
        "                        gts   = bmi.flatten().tolist()\n",
        "                        buckets = [get_bmi_class_new(float(x)) for x in gts]\n",
        "                        all_preds   += preds\n",
        "                        all_gts     += gts\n",
        "                        all_buckets += buckets\n",
        "\n",
        "                # compute overall metrics\n",
        "                metrics = compute_metrics(all_gts, all_preds)\n",
        "                print(f\"\\n--- Epoch {epoch} metrics ---\")\n",
        "                for k,v in metrics.items():\n",
        "                    print(f\"{k}: {v:.4f}\", end=\"  \")\n",
        "                print()\n",
        "\n",
        "                # append to overall results\n",
        "                for k,v in metrics.items():\n",
        "                    overall_results[epoch][k].append(v)\n",
        "\n",
        "                # compute and append bucket MAEs\n",
        "                errors = np.abs(np.array(all_preds) - np.array(all_gts))\n",
        "                arr_buckets = np.array(all_buckets)\n",
        "                for b in (0,1,2):\n",
        "                    mask = (arr_buckets == b)\n",
        "                    mae = errors[mask].mean() if mask.sum()>0 else float('nan')\n",
        "                    bucket_results[epoch][b].append(mae)\n",
        "\n",
        "                # visualize for this run/checkpoint\n",
        "                names = [\"Underweight\",\"Normal\",\"Overweight\"]\n",
        "                bucket_mae = [bucket_results[epoch][b][-1] for b in (0,1,2)]\n",
        "                plt.figure(figsize=(6,4))\n",
        "                plt.bar(names, bucket_mae)\n",
        "                plt.ylabel(\"MAE\")\n",
        "                plt.title(f\"MAE by BMI Bucket @ epoch {epoch}\")\n",
        "                plt.show()\n",
        "\n",
        "    # at end, print meanÂ±std for both overall and buckets\n",
        "    print(\"\\n===== OVERALL METRIC SUMMARY =====\")\n",
        "    for ep in CHECKPOINTS:\n",
        "        print(f\"\\nCheckpoint {ep}:\")\n",
        "        for k, vals in overall_results[ep].items():\n",
        "            arr = np.array(vals)\n",
        "            print(f\"  {k.upper():12s} {arr.mean():.4f} Â± {arr.std():.4f}\")\n",
        "        print(\"  BUCKET MAE:\")\n",
        "        for b,name in zip((0,1,2), [\"Underweight\",\"Normal\",\"Overweight\"]):\n",
        "            arr = np.array(bucket_results[ep][b])\n",
        "            print(f\"    {name:12s}: {arr.mean():.4f} Â± {arr.std():.4f}\")\n"
      ]
    }
  ]
}