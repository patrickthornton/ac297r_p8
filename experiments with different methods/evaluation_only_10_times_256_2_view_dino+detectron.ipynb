{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bIWn6j9DJtMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n"
      ],
      "metadata": {
        "id": "h_fUyNRJJq9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2TdzRiGJmqL"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "multiprocessing.set_start_method(\"spawn\", force=True)\n",
        "\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "import time\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageOps\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Optional, Union, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from scipy.stats import ks_2samp, wasserstein_distance\n",
        "\n",
        "# Mount Google Drive (ensure JSON files and images are stored in Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# -------------------------------\n",
        "# Global device configuration\n",
        "# -------------------------------\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Current device:\", DEVICE)\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load the Detectron2 model (for human detection)\n",
        "# -------------------------------\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Detection threshold\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "person_predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Load the Grounding DINO model (for reference object detection without using SAM)\n",
        "# -------------------------------\n",
        "from transformers import pipeline\n",
        "# Initialize globally to avoid repeated instantiation\n",
        "GLOBAL_GROUNDING_DETECTOR = pipeline(\n",
        "    model=\"IDEA-Research/grounding-dino-tiny\",\n",
        "    task=\"zero-shot-object-detection\",\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "print(\"Grounding Dino global initialization complete.\")\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Define data structures\n",
        "# -------------------------------\n",
        "@dataclass\n",
        "class BoundingBox:\n",
        "    xmin: int\n",
        "    ymin: int\n",
        "    xmax: int\n",
        "    ymax: int\n",
        "\n",
        "    @property\n",
        "    def xyxy(self) -> List[float]:\n",
        "        return [self.xmin, self.ymin, self.xmax, self.ymax]\n",
        "\n",
        "@dataclass\n",
        "class DetectionResult:\n",
        "    score: float\n",
        "    label: str\n",
        "    box: BoundingBox\n",
        "    mask: Optional[np.ndarray] = None   # SAM mask is not used here\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, detection_dict: Dict) -> 'DetectionResult':\n",
        "        return cls(\n",
        "            score=detection_dict['score'],\n",
        "            label=detection_dict['label'],\n",
        "            box=BoundingBox(\n",
        "                xmin=int(detection_dict['box']['xmin']),\n",
        "                ymin=int(detection_dict['box']['ymin']),\n",
        "                xmax=int(detection_dict['box']['xmax']),\n",
        "                ymax=int(detection_dict['box']['ymax'])\n",
        "            )\n",
        "        )\n",
        "\n",
        "# -------------------------------\n",
        "# Reference object detection function (using Grounding DINO only, without SAM)\n",
        "# -------------------------------\n",
        "def grounded_segmentation(image: Union[Image.Image, str],\n",
        "                          labels: List[str],\n",
        "                          threshold: float = 0.7,\n",
        "                          polygon_refinement: bool = False,\n",
        "                          detector_id: Optional[str] = None) -> Tuple[np.ndarray, List[DetectionResult]]:\n",
        "    \"\"\"\n",
        "    Use Grounding DINO to detect specified reference objects in the image (without calling SAM),\n",
        "    returning the EXIF-corrected image (as a numpy array) and a list of detection results.\n",
        "    The detection results contain only bounding boxes (mask is always None).\n",
        "    \"\"\"\n",
        "    if isinstance(image, str):\n",
        "        image = Image.open(image).convert(\"RGB\")\n",
        "    image = ImageOps.exif_transpose(image)\n",
        "    labels = [lb if lb.endswith('.') else lb + '.' for lb in labels]\n",
        "    results = GLOBAL_GROUNDING_DETECTOR(image, candidate_labels=labels, threshold=threshold)\n",
        "    detections = [DetectionResult.from_dict(r) for r in results]\n",
        "    return np.array(image), detections\n",
        "\n",
        "# -------------------------------\n",
        "# 4. \"View Feature Extraction\" Function (2-view, reference feature output dimension 5*2=10)\n",
        "# -------------------------------\n",
        "def compute_view_features(img_obj: Image.Image, ref_labels: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    For an image:\n",
        "      - Use Grounding DINO to detect reference objects (without calling SAM); if detected, set flag=1,\n",
        "        and area ratio = (bounding box area)/(image area); otherwise, both are 0.\n",
        "      - Use Detectron2 to detect humans; if a mask is available, compute area ratio using the mask,\n",
        "        otherwise use the bounding box area.\n",
        "    Returns a 5-dimensional feature vector.\n",
        "    \"\"\"\n",
        "    W, H = img_obj.width, img_obj.height\n",
        "    # Reference object detection (using DINO only, returning bounding boxes)\n",
        "    _, detections = grounded_segmentation(img_obj, ref_labels, threshold=0.7, detector_id=\"IDEA-Research/grounding-dino-tiny\")\n",
        "    feats_ref = []\n",
        "    for label in ref_labels:\n",
        "        matched = [d for d in detections if d.label.strip('.').lower() == label.strip('.').lower()]\n",
        "        if matched:\n",
        "            best = max(matched, key=lambda d: d.score)\n",
        "            box = best.box.xyxy\n",
        "            bbox_area = (box[2] - box[0]) * (box[3] - box[1])\n",
        "            ratio = bbox_area / (W * H)\n",
        "            flag = 1.0\n",
        "        else:\n",
        "            flag = 0.0\n",
        "            ratio = 0.0\n",
        "        feats_ref.extend([flag, ratio])\n",
        "    # Human detection using Detectron2\n",
        "    img_np = np.array(img_obj)\n",
        "    img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "    outputs = person_predictor(img_cv)\n",
        "    instances = outputs[\"instances\"]\n",
        "    human_ratio = 0.0\n",
        "    if len(instances) > 0:\n",
        "        person_idxs = (instances.pred_classes == 0).nonzero().squeeze()\n",
        "        if person_idxs.numel() > 0:\n",
        "            if person_idxs.ndim == 0:\n",
        "                idx = person_idxs.item()\n",
        "            else:\n",
        "                person_scores = instances.scores[person_idxs]\n",
        "                idx = person_idxs[person_scores.argmax()]\n",
        "            if instances.has(\"pred_masks\") and len(instances.pred_masks) > idx:\n",
        "                person_mask = instances.pred_masks[idx].cpu().numpy().astype(np.uint8)\n",
        "                human_ratio = float(np.sum(person_mask)) / (W * H)\n",
        "            else:\n",
        "                bbox = instances.pred_boxes[idx].tensor.cpu().numpy()[0]\n",
        "                bbox_area = (bbox[2] - bbox[0]) * (bbox[3] - bbox[1])\n",
        "                human_ratio = bbox_area / (W * H)\n",
        "    feats = np.array(feats_ref + [human_ratio], dtype=np.float32)\n",
        "    return feats  # Output is a 5-dimensional feature vector\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Offline Precomputation of Features (each sample has 2 views → total dimension 10)\n",
        "# -------------------------------\n",
        "def offline_precompute_features_chunked(samples, cache_path, ref_labels=[\"a black paper\", \"a white ball\"], chunk_size=30):\n",
        "    \"\"\"\n",
        "    For each sample (requiring at least 2 images), compute the 5-dimensional feature for each image,\n",
        "    then for each sample, take the first 2 view features and concatenate them into a 10-dimensional feature.\n",
        "    The result is saved to cache_path.\n",
        "    \"\"\"\n",
        "    print(f\"\\n[Offline Precomputation - Chunked] Total samples: {len(samples)}\")\n",
        "    print(f\"Cache file: {cache_npy}\")\n",
        "    print(f\"chunk_size={chunk_size}, ref_labels={ref_labels}\\n\")\n",
        "    feat_dim_per_img = 5\n",
        "    total_feat_dim = feat_dim_per_img * 2  # 2 views → 10 dimensions\n",
        "    feature_cache = np.zeros((len(samples), total_feat_dim), dtype=np.float32)\n",
        "    num_samples = len(samples)\n",
        "    start_idx = 0\n",
        "    while start_idx < num_samples:\n",
        "        end_idx = min(start_idx + chunk_size, num_samples)\n",
        "        print(f\"=== Processing sample indices [{start_idx}, {end_idx}) ===\")\n",
        "        chunk = samples[start_idx:end_idx]\n",
        "        chunk_img_paths = []\n",
        "        index_map = []  # (local index within the chunk, which image) -- only use the first 2 views\n",
        "        for local_i, s in enumerate(chunk):\n",
        "            paths = s[\"img_paths\"]\n",
        "            if len(paths) < 2:\n",
        "                raise ValueError(f\"Each sample must have at least 2 images, but found {len(paths)} in {paths}\")\n",
        "            for j in range(2):\n",
        "                chunk_img_paths.append(paths[j])\n",
        "                index_map.append((local_i, j))\n",
        "        chunk_images = []\n",
        "        for p in chunk_img_paths:\n",
        "            if not os.path.exists(p):\n",
        "                print(f\"❌ Image not found: {p}\")\n",
        "                chunk_images.append(None)\n",
        "            else:\n",
        "                try:\n",
        "                    img = Image.open(p).convert(\"RGB\")\n",
        "                    chunk_images.append(img)\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ Unable to open image: {p} | Error: {e}\")\n",
        "                    chunk_images.append(None)\n",
        "        chunk_sample_feats = [[None, None] for _ in range(len(chunk))]\n",
        "        for global_img_idx, (local_i, which_img) in enumerate(index_map):\n",
        "            img_obj = chunk_images[global_img_idx]\n",
        "            if img_obj is None:\n",
        "                chunk_sample_feats[local_i][which_img] = np.zeros(feat_dim_per_img, dtype=np.float32)\n",
        "            else:\n",
        "                feats = compute_view_features(img_obj, ref_labels)\n",
        "                chunk_sample_feats[local_i][which_img] = feats\n",
        "        for local_i in range(len(chunk)):\n",
        "            feat_img0 = chunk_sample_feats[local_i][0]\n",
        "            feat_img1 = chunk_sample_feats[local_i][1]\n",
        "            final_feat = np.concatenate([feat_img0, feat_img1], axis=0)  # 5+5=10 dimensions\n",
        "            real_sample_idx = start_idx + local_i\n",
        "            feature_cache[real_sample_idx] = final_feat\n",
        "        del chunk_images\n",
        "        start_idx = end_idx\n",
        "    np.save(cache_path, feature_cache)\n",
        "    print(f\"\\n[Chunk Extraction Complete] Features saved to: {cache_path}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Datasets and Model (2-view version)\n",
        "# -------------------------------\n",
        "class LetterboxResize:\n",
        "    def __init__(self, size=(224, 224), fill=(0.485, 0.456, 0.406)):\n",
        "        self.size = size\n",
        "        self.fill = fill\n",
        "    def __call__(self, img):\n",
        "        iw, ih = img.size\n",
        "        w, h = self.size\n",
        "        scale = min(w / iw, h / ih)\n",
        "        nw, nh = int(iw * scale), int(ih * scale)\n",
        "        img = img.resize((nw, nh), Image.BICUBIC)\n",
        "        new_img = Image.new(\"RGB\", self.size, tuple([int(c * 255) for c in self.fill]))\n",
        "        new_img.paste(img, ((w - nw) // 2, (h - nh) // 2))\n",
        "        return new_img\n",
        "\n",
        "class MultiViewBMIWithRefDataset(Dataset):\n",
        "    def __init__(self, samples, feature_cache, transform=None):\n",
        "        self.samples = samples\n",
        "        self.feature_cache = feature_cache  # shape: (number of samples, 10)\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        img_paths = sample[\"img_paths\"]\n",
        "        bmi = float(sample[\"bmi\"])\n",
        "        imgs = []\n",
        "        # Only use the first 2 views\n",
        "        for j in range(2):\n",
        "            p = img_paths[j]\n",
        "            if not os.path.exists(p):\n",
        "                img = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
        "            else:\n",
        "                try:\n",
        "                    img = Image.open(p).convert(\"RGB\")\n",
        "                except Exception as e:\n",
        "                    print(f\"❌ [Dataset] Unable to open image: {p} | Error: {e}\")\n",
        "                    img = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
        "            if self.transform is not None:\n",
        "                img = self.transform(img)\n",
        "            imgs.append(img)\n",
        "        ref_feats_np = self.feature_cache[idx]  # 10 dimensions\n",
        "        ref_feats_tensor = torch.tensor(ref_feats_np, dtype=torch.float32)\n",
        "        bmi_tensor = torch.tensor(bmi, dtype=torch.float32)\n",
        "        return imgs[0], imgs[1], ref_feats_tensor, bmi_tensor\n",
        "\n",
        "class FusedMultiViewBMIModel(nn.Module):\n",
        "    def __init__(self, ref_dim=256, num_classes=4):\n",
        "        super().__init__()\n",
        "        # Load ResNet101 (excluding the fully connected layers)\n",
        "        base = models.resnet101(weights=None)\n",
        "        resnet_path = \"/content/drive/MyDrive/resnet101-63fe2227.pth\"\n",
        "        state_dict = torch.load(resnet_path, map_location=DEVICE)\n",
        "        base.load_state_dict(state_dict)\n",
        "        self.backbone = nn.Sequential(*list(base.children())[:-1])\n",
        "        # Reference feature projection: input is 10 dimensions, output is ref_dim\n",
        "        self.ref_projection = nn.Sequential(\n",
        "            nn.Linear(10, ref_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # 2 views: each with 2048 dimensions; concatenated yields 4096, then add ref_dim\n",
        "        self.fc_reg = nn.Linear(4096 + ref_dim, 1)\n",
        "        self.fc_cls = nn.Linear(4096 + ref_dim, num_classes)\n",
        "    def forward(self, img0, img1, ref_features):\n",
        "        x0 = self.backbone(img0).flatten(1)\n",
        "        x1 = self.backbone(img1).flatten(1)\n",
        "        fused_cnn = torch.cat([x0, x1], dim=1)  # 4096 dimensions\n",
        "        ref_proj = self.ref_projection(ref_features)\n",
        "        fused_all = torch.cat([fused_cnn, ref_proj], dim=1)\n",
        "        out_reg = self.fc_reg(fused_all)\n",
        "        out_cls = self.fc_cls(fused_all)\n",
        "        return out_reg, out_cls\n",
        "\n",
        "# -------------------------------\n",
        "# Main Process: Offline feature precomputation → Data loading → Model training and validation\n",
        "# -------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    sample_json = \"/content/drive/MyDrive/sample_list.json\"\n",
        "    cache_npy = \"/content/drive/MyDrive/features_cache_2view.npy\"  # Offline features: 10 dimensions\n",
        "    ref_labels = [\"a black paper\", \"a white ball\"]\n",
        "\n",
        "    with open(sample_json, \"r\") as f:\n",
        "        samples = json.load(f)\n",
        "\n",
        "    # Check if the npy cache file exists; if so, skip recomputation\n",
        "    if not os.path.exists(cache_npy):\n",
        "        print(f\"Cache file {cache_npy} does not exist, starting offline feature computation...\")\n",
        "        offline_precompute_features_chunked(\n",
        "            samples=samples,\n",
        "            cache_path=cache_npy,\n",
        "            ref_labels=ref_labels,\n",
        "            chunk_size=30\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Cache file {cache_npy} found, skipping offline feature computation.\")\n",
        "    feature_cache = np.load(cache_npy)  # shape: (number of samples, 10)\n",
        "\n",
        "    train_samples = [s for s in samples if s[\"split\"] == \"Training\"]\n",
        "    val_samples = [s for s in samples if s[\"split\"] == \"Validation\"]\n",
        "    train_indices = [i for i, s in enumerate(samples) if s[\"split\"] == \"Training\"]\n",
        "    val_indices = [i for i, s in enumerate(samples) if s[\"split\"] == \"Validation\"]\n",
        "\n",
        "    train_features = feature_cache[train_indices]\n",
        "    val_features = feature_cache[val_indices]\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        LetterboxResize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    train_dataset = MultiViewBMIWithRefDataset(train_samples, train_features, transform=transform)\n",
        "    val_dataset = MultiViewBMIWithRefDataset(val_samples, val_features, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0, pin_memory=True)\n",
        "\n",
        "    print(f\"Train Dataset Size: {len(train_dataset)}\")\n",
        "    print(f\"Val Dataset Size: {len(val_dataset)}\")\n",
        "    print(f\"Train Loader Batch Count: {len(train_loader)}\")\n",
        "    print(f\"Val Loader Batch Count: {len(val_loader)}\")\n",
        "\n",
        "    # New training strategy: train continuously for each experiment until max_epoch (max checkpoint epoch),\n",
        "    # recording validation metrics at checkpoints (20, 25, 30, 35)\n",
        "    checkpoint_epochs = [20, 25, 30, 35]\n",
        "    max_epoch = max(checkpoint_epochs)\n",
        "    num_runs = 5\n",
        "\n",
        "    # 'results' stores metrics from multiple experiments for each checkpoint\n",
        "    results = {ep: {'r2': [], 'mae': [], 'tol_rate': [], 'mean_bias': [], 'error_std': [], 'ks': [], 'wasserstein': []}\n",
        "               for ep in checkpoint_epochs}\n",
        "\n",
        "    for run in range(num_runs):\n",
        "        print(f\"\\n==== Running experiment {run+1} ====\")\n",
        "        model = FusedMultiViewBMIModel(ref_dim=256, num_classes=4).to(DEVICE)\n",
        "        criterion_reg = nn.MSELoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "        # Train continuously for max_epoch epochs in each experiment\n",
        "        for epoch in range(1, max_epoch + 1):\n",
        "            start_time = time.time()\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            for (img0, img1, ref_feats, bmi) in train_loader:\n",
        "                img0, img1 = img0.to(DEVICE), img1.to(DEVICE)\n",
        "                ref_feats = ref_feats.to(DEVICE)\n",
        "                bmi = bmi.to(DEVICE).unsqueeze(1)\n",
        "                optimizer.zero_grad()\n",
        "                out_reg, out_cls = model(img0, img1, ref_feats)\n",
        "                loss = criterion_reg(out_reg, bmi)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item() * bmi.size(0)\n",
        "            epoch_loss = running_loss / len(train_dataset)\n",
        "            print(f\"[Run {run+1}] Epoch {epoch}/{max_epoch}, Loss: {epoch_loss:.4f}, Time: {time.time()-start_time:.2f}s\")\n",
        "\n",
        "            # If the current epoch is a checkpoint, perform validation and record metrics\n",
        "            if epoch in checkpoint_epochs:\n",
        "                model.eval()\n",
        "                all_preds, all_targets = [], []\n",
        "                with torch.no_grad():\n",
        "                    for (img0, img1, ref_feats, bmi) in val_loader:\n",
        "                        img0, img1 = img0.to(DEVICE), img1.to(DEVICE)\n",
        "                        ref_feats = ref_feats.to(DEVICE)\n",
        "                        bmi = bmi.to(DEVICE).unsqueeze(1)\n",
        "                        out_reg, _ = model(img0, img1, ref_feats)\n",
        "                        all_preds.extend(out_reg.cpu().numpy().flatten().tolist())\n",
        "                        all_targets.extend(bmi.cpu().numpy().flatten().tolist())\n",
        "                gt_arr = np.array(all_targets)\n",
        "                pred_arr = np.array(all_preds)\n",
        "                try:\n",
        "                    r2_val = r2_score(gt_arr, pred_arr)\n",
        "                except Exception as e:\n",
        "                    r2_val = 0.0\n",
        "                mae_val = mean_absolute_error(gt_arr, pred_arr)\n",
        "                tolerance = 1.0\n",
        "                tol_rate = np.mean(np.abs(gt_arr - pred_arr) <= tolerance)\n",
        "                error = pred_arr - gt_arr\n",
        "                mean_bias = np.mean(error)\n",
        "                error_std = np.std(error)\n",
        "                try:\n",
        "                    ks_val, _ = ks_2samp(gt_arr, pred_arr)\n",
        "                    wass_val = wasserstein_distance(gt_arr, pred_arr)\n",
        "                except Exception as e:\n",
        "                    ks_val = wass_val = 0.0\n",
        "\n",
        "                print(f\"\\n[Run {run+1}] Epoch {epoch} Validation Metrics:\")\n",
        "                print(f\"R²: {r2_val:.4f}\")\n",
        "                print(f\"MAE: {mae_val:.4f}\")\n",
        "                print(f\"Tolerance Rate (±{tolerance}): {tol_rate*100:.2f}%\")\n",
        "                print(f\"Mean Bias: {mean_bias:.4f}\")\n",
        "                print(f\"Error Std: {error_std:.4f}\")\n",
        "                print(f\"KS: {ks_val:.4f}\")\n",
        "                print(f\"Wasserstein: {wass_val:.4f}\\n\")\n",
        "\n",
        "                # Save the metrics for this checkpoint\n",
        "                results[epoch]['r2'].append(r2_val)\n",
        "                results[epoch]['mae'].append(mae_val)\n",
        "                results[epoch]['tol_rate'].append(tol_rate)\n",
        "                results[epoch]['mean_bias'].append(mean_bias)\n",
        "                results[epoch]['error_std'].append(error_std)\n",
        "                results[epoch]['ks'].append(ks_val)\n",
        "                results[epoch]['wasserstein'].append(wass_val)\n",
        "\n",
        "                # Optional: Plot validation result figures\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                plt.scatter(gt_arr, pred_arr, alpha=0.6, label=\"Predictions\")\n",
        "                cp_min = min(np.min(gt_arr), np.min(pred_arr))\n",
        "                cp_max = max(np.max(gt_arr), np.max(pred_arr))\n",
        "                plt.plot([cp_min, cp_max], [cp_min, cp_max], \"r--\", label=\"Ideal\")\n",
        "                plt.xlabel(\"Ground Truth BMI\")\n",
        "                plt.ylabel(\"Predicted BMI\")\n",
        "                plt.title(f\"Run {run+1} Epoch {epoch}: Scatter Plot\")\n",
        "                plt.legend()\n",
        "                plt.show()\n",
        "\n",
        "                bins = np.linspace(cp_min, cp_max, 20)\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                plt.hist(gt_arr, bins=bins, alpha=0.5, label=\"Ground Truth\")\n",
        "                plt.hist(pred_arr, bins=bins, alpha=0.5, label=\"Predictions\")\n",
        "                plt.xlabel(\"BMI\")\n",
        "                plt.ylabel(\"Count\")\n",
        "                plt.title(f\"Run {run+1} Epoch {epoch}: Distribution Histogram\")\n",
        "                plt.legend()\n",
        "                plt.show()\n",
        "\n",
        "    # Output the aggregated statistics over all runs for each checkpoint\n",
        "    print(\"\\n========== Overall Results Summary ==========\")\n",
        "    for ep in sorted(results.keys()):\n",
        "        print(f\"\\nCheckpoint Epoch: {ep}\")\n",
        "        metrics = results[ep]\n",
        "        for key in metrics:\n",
        "            m = np.mean(metrics[key])\n",
        "            s = np.std(metrics[key])\n",
        "            print(f\"{key.upper()}: Mean = {m:.4f}, Std = {s:.4f}\")\n"
      ]
    }
  ]
}