{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDTeYwuD8XVt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import ResNet101_Weights\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Exponentiation, RationalQuadratic\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "DATASET_PATHS = {\"train\": \"/content/drive/MyDrive/Image_train/\", \"val\": \"/content/drive/MyDrive/Image_val/\"}\n",
        "CSV_PATHS = {\"train\": \"/content/drive/MyDrive/Image_train_features_new.csv\", \"val\": \"/content/drive/MyDrive/Image_val_features_new.csv\"}\n",
        "# DATASET_PATHS = {\"train\": \"/content/drive/MyDrive/Image_train_gaussian_15/\", \"val\": \"/content/drive/MyDrive/Image_val_gaussian_15/\"}\n",
        "# CSV_PATHS = {\"train\": \"/content/drive/MyDrive/Image_train_features_new_gaussian_15.csv\", \"val\": \"/content/drive/MyDrive/Image_val_features_new_gaussian_15.csv\"}\n",
        "# DATASET_PATHS = {\"train\": \"/content/drive/MyDrive/Image_train_gaussian_30/\", \"val\": \"/content/drive/MyDrive/Image_val_gaussian_30/\"}\n",
        "# CSV_PATHS = {\"train\": \"/content/drive/MyDrive/Image_train_features_new_gaussian_30.csv\", \"val\": \"/content/drive/MyDrive/Image_val_features_new_gaussian_30.csv\"}\n",
        "# DATASET_PATHS = {\"train\": \"/content/drive/MyDrive/Image_train_rotate_10/\", \"val\": \"/content/drive/MyDrive/Image_val_rotate_10/\"}\n",
        "# CSV_PATHS = {\"train\": \"/content/drive/MyDrive/Image_train_features_new_rotate_10.csv\", \"val\": \"/content/drive/MyDrive/Image_val_features_new_rotate_10.csv\"}\n",
        "# DATASET_PATHS = {\"train\": \"/content/drive/MyDrive/Image_train_rotate_20/\", \"val\": \"/content/drive/MyDrive/Image_val_rotate_20/\"}\n",
        "# CSV_PATHS = {\"train\": \"/content/drive/MyDrive/Image_train_features_new_rotate_20.csv\", \"val\": \"/content/drive/MyDrive/Image_val_features_new_rotate_20.csv\"}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "def extract_bmi_from_filename(filename):\n",
        "    match = re.match(r\"\\d+?_([FMfm])_(\\d+?)_(\\d+?)_(\\d+).+\", filename)\n",
        "    if not match:\n",
        "        print(f\"Skipping invalid filename: {filename}\")\n",
        "        return None, None, None\n",
        "    try:\n",
        "        height = int(match.group(3)) / 100000\n",
        "        weight = int(match.group(4)) / 100000\n",
        "        bmi = weight / (height ** 2)\n",
        "        return height, weight, bmi\n",
        "    except ValueError:\n",
        "        print(f\"Error parsing BMI from filename: {filename}\")\n",
        "        return None, None, None\n",
        "\n",
        "class BMIDataset(Dataset):\n",
        "    def __init__(self, folder, transform=None):\n",
        "        self.folder = folder\n",
        "        self.image_files = [f for f in os.listdir(folder) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.folder, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        height, weight, bmi = extract_bmi_from_filename(img_name)\n",
        "        if height is None:\n",
        "            bmi = 0.0\n",
        "        return image, float(bmi)\n",
        "\n",
        "train_dataset = BMIDataset(DATASET_PATHS[\"train\"], transform=transform)\n",
        "val_dataset   = BMIDataset(DATASET_PATHS[\"val\"], transform=transform)\n",
        "train_loader  = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader    = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "class DFNet(nn.Module):\n",
        "    def __init__(self, num_features=32):\n",
        "        super(DFNet, self).__init__()\n",
        "        self.resnet = models.resnet101(weights=ResNet101_Weights.IMAGENET1K_V1)\n",
        "        in_feats = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Identity()\n",
        "        self.fc1 = nn.Linear(in_feats, num_features)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.regressor = nn.Linear(num_features, 1)\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        features = self.relu(self.fc1(x))\n",
        "        out = self.regressor(features)\n",
        "        return out, features\n",
        "model_nn = DFNet(num_features=32).to(DEVICE)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_nn.parameters(), lr=1e-4)\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model_nn.train()\n",
        "    running_loss = 0.0\n",
        "    for images, targets in train_loader:\n",
        "        images = images.to(DEVICE)\n",
        "        targets = torch.as_tensor(targets, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model_nn(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "model_nn.eval()\n",
        "# all_preds = []\n",
        "# all_targets = []\n",
        "# with torch.no_grad():\n",
        "#     for images, targets in val_loader:\n",
        "#         images = images.to(DEVICE)\n",
        "#         targets = torch.as_tensor(targets, dtype=torch.float32, device=DEVICE).unsqueeze(1)\n",
        "#         outputs, _ = model_nn(images)\n",
        "#         all_preds.extend(outputs.cpu().numpy().flatten().tolist())\n",
        "#         all_targets.extend(targets.cpu().numpy().flatten().tolist())\n",
        "# val_mae = mean_absolute_error(all_targets, all_preds)\n",
        "# print(f\"DFNet Validation MAE: {val_mae:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_human_features(csv_path):\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(csv_path)\n",
        "    feature_dict = {}\n",
        "    for _, row in df.iterrows():\n",
        "        filename = row[\"Filename\"]\n",
        "        features = [float(row[f\"Anthro_{i+1}\"]) for i in range(7)]\n",
        "        feature_dict[filename] = features\n",
        "    return feature_dict\n",
        "\n",
        "def extract_features(image_path, model, human_features_dict):\n",
        "    img_e = cv2.imread(image_path)\n",
        "    if img_e is None:\n",
        "        print(f\"Failed to read image: {image_path}\")\n",
        "        return None\n",
        "    filename = os.path.basename(image_path)\n",
        "    anthro_feats = human_features_dict.get(filename, [0.0]*7)\n",
        "    pil_img = Image.fromarray(cv2.cvtColor(img_e, cv2.COLOR_BGR2RGB))\n",
        "    img_tensor = transform(pil_img).unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        _, deep_feats = model(img_tensor)\n",
        "        deep_feats = torch.squeeze(deep_feats.cpu().detach()).numpy().tolist()\n",
        "    torch.cuda.empty_cache()\n",
        "    return anthro_feats + deep_feats\n",
        "BATCH_SIZE = 10\n",
        "def process_dataset(dataset_type, model, human_features_dict):\n",
        "    image_folder = DATASET_PATHS[dataset_type]\n",
        "    output_csv = CSV_PATHS[dataset_type]\n",
        "    image_files = [f for f in os.listdir(image_folder) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    total_images = len(image_files)\n",
        "    if total_images == 0:\n",
        "        print(f\"No images found in {image_folder}\")\n",
        "        return\n",
        "    with open(output_csv, mode=\"w\", newline=\"\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        header = [\"Filename\", \"Height\", \"Weight\", \"BMI\"] + \\\n",
        "                 [f\"Anthro_{i+1}\" for i in range(7)] + [f\"DeepFeature_{i+1}\" for i in range(32)]\n",
        "        writer.writerow(header)\n",
        "        for batch_start in range(0, total_images, BATCH_SIZE):\n",
        "            batch_files = image_files[batch_start: batch_start+BATCH_SIZE]\n",
        "            batch_results = []\n",
        "            for img_name in batch_files:\n",
        "                img_path = os.path.join(image_folder, img_name)\n",
        "                height, weight, bmi = extract_bmi_from_filename(img_name)\n",
        "                if height is None:\n",
        "                    continue\n",
        "                feats = extract_features(img_path, model, human_features_dict)\n",
        "                if feats is None:\n",
        "                    continue\n",
        "                batch_results.append([img_name, height, weight, bmi] + feats)\n",
        "            writer.writerows(batch_results)\n",
        "            torch.cuda.empty_cache()\n",
        "            progress = min(batch_start+BATCH_SIZE, total_images)\n",
        "            print(f\"Processed {progress}/{total_images} images in {dataset_type} ({progress/total_images:.0%})\")\n",
        "    print(f\"Feature extraction complete for {dataset_type}! Saved to {output_csv}\")\n",
        "\n",
        "train_human_features = load_human_features(\"/content/drive/MyDrive/Image_train_features.csv\")\n",
        "val_human_features = load_human_features(\"/content/drive/MyDrive/Image_val_features.csv\")\n",
        "# train_human_features = load_human_features(\"/content/drive/MyDrive/Image_train_gaussian_15_features.csv\")\n",
        "# val_human_features = load_human_features(\"/content/drive/MyDrive/Image_val_gaussian_15_features.csv\")\n",
        "# train_human_features = load_human_features(\"/content/drive/MyDrive/Image_train_gaussian_30_features.csv\")\n",
        "# val_human_features = load_human_features(\"/content/drive/MyDrive/Image_val_gaussian_30_features.csv\")\n",
        "# train_human_features = load_human_features(\"/content/drive/MyDrive/Image_train_rotate_10_features.csv\")\n",
        "# val_human_features = load_human_features(\"/content/drive/MyDrive/Image_val_rotate_10_features.csv\")\n",
        "# train_human_features = load_human_features(\"/content/drive/MyDrive/Image_train_rotate_20_features.csv\")\n",
        "# val_human_features = load_human_features(\"/content/drive/MyDrive/Image_val_rotate_20_features.csv\")\n",
        "\n",
        "process_dataset(\"train\", model_nn, train_human_features)\n",
        "process_dataset(\"val\", model_nn, val_human_features)\n",
        "\n",
        "\n",
        "train_data = np.loadtxt(CSV_PATHS[\"train\"], delimiter=\",\", skiprows=1, usecols=range(1, 4+7+32))\n",
        "val_data = np.loadtxt(CSV_PATHS[\"val\"], delimiter=\",\", skiprows=1, usecols=range(1, 4+7+32))\n",
        "y_train = train_data[:, 2]\n",
        "x_7f_train = train_data[:, 3:10]\n",
        "x_df_train = train_data[:, 10:]\n",
        "y_val = val_data[:, 2]\n",
        "x_7f_val = val_data[:, 3:10]\n",
        "x_df_val = val_data[:, 10:]\n",
        "Mean = np.mean(x_7f_train, axis=0)\n",
        "Std = np.std(x_7f_train, axis=0)\n",
        "x_7f_train_norm = (x_7f_train - Mean) / Std\n",
        "x_7f_val_norm = (x_7f_val - Mean) / Std\n",
        "x_train = np.concatenate([x_7f_train_norm, x_df_train], axis=1)\n",
        "x_val = np.concatenate([x_7f_val_norm, x_df_val], axis=1)\n",
        "kernel = Exponentiation(RationalQuadratic(), exponent=2)\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, alpha=1e-3)\n",
        "gpr.fit(x_train, y_train)\n",
        "y_pred = gpr.predict(x_val)\n",
        "mae = mean_absolute_error(y_val, y_pred)\n",
        "print(f\"GPR Validation MAE: {mae:.4f}\")\n"
      ],
      "metadata": {
        "id": "d7vb49ki8h0g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}